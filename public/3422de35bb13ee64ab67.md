---
title: 教師有学習の回帰曲線に関する応用について
tags:
  - 機械学習
  - データサイエンス
private: false
updated_at: '2024-08-24T18:05:42+09:00'
id: 3422de35bb13ee64ab67
organization_url_name: null
slide: false
ignorePublish: false
---
# 下記曲線の応用について
未知データにおける予測と実測値の誤差は
 ・バリアランス
　・バイアス
　・ノイズ
に分解可能である。

機械学習には「**過学習**」という現象があるが、これは上記の**バリアランス**によるものである。
バリアランス：予測結果の分散のこと。

過学習の減少を回避する為に、回帰分析の中で二つご紹介する。
### リッジ回帰
リッジ回帰は「回帰曲線」の係数をできる限り小さくするというコンセプトのもと成り立っている回帰モデルである。

実際に、**sklearn.linear_model**からモジュールをインプットして使用する際は下記のように使用する。

```python:test.sh
from sklearn.linear_model import Ridge # モジュールインポート
# モデルの作成
ridgeModel = Ridge(alpha = 10)
ridgeModel.fit(x_train, y_train) # 学習
print(ridgeModel.score(x_train, y_train))
print(ridgeModel.score(x_test, y_test))
```


### ラッソ回帰
リッジ回帰とは別にラッソ回帰という回帰モデルがある。このモデルは、回帰曲線の**係数の絶対値の合計**が最小化するようにするモデルである。

実際のコードサンプルは下記の通り。
```python:test2.sh
from sklearn.linear_model import Lasso

x_train, x_test, y_train, y_test = train_test_split(pf_x,
    sc_t, test_size = 0.3, random_state = 0)

# ラッソ回帰のモデル作成（alphaは正則化項につく定数）
model = Lasso(alpha = 0.1)
model.fit(x_train, y_train)

print(model.score(x_train, y_train)) # 訓練データの決定係数
print(model.score(x_test, y_test)) # テストデータの決定係数
```
